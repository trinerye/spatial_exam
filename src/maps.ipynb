{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "### remember to make a seperate notebook on cleaning the data, i.e. removing un-needed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas openpyxl geopandas matplotlib\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Geographic 2D CRS: EPSG:4326>\n",
       "Name: WGS 84\n",
       "Axis Info [ellipsoidal]:\n",
       "- Lat[north]: Geodetic latitude (degree)\n",
       "- Lon[east]: Geodetic longitude (degree)\n",
       "Area of Use:\n",
       "- name: World.\n",
       "- bounds: (-180.0, -90.0, 180.0, 90.0)\n",
       "Datum: World Geodetic System 1984 ensemble\n",
       "- Ellipsoid: WGS 84\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_file = os.path.join('..','in','datacenters_usa_clean.csv')\n",
    "\n",
    "out_path = os.path.join('..','out')\n",
    "\n",
    "#dc_df = datacenter_dataframe\n",
    "dc_df = pd.read_csv(in_file)\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "dc_gdf = gpd.GeoDataFrame(dc_df, geometry=gpd.points_from_xy(dc_df.longitude, dc_df.latitude))\n",
    "\n",
    "# Setting the coordinate reference system (CRS)\n",
    "# For the United States, we can use EPSG:4326 (WGS 84) which is commonly used for latitude and longitude coordinates\n",
    "dc_gdf.crs = \"EPSG:4326\"\n",
    "\n",
    "shapefile_path = os.path.join('..','in','States_shapefile-shp','States_shapefile.shp')\n",
    "\n",
    "# Loading custom us state shapefile\n",
    "states_map = gpd.read_file(shapefile_path)\n",
    "\n",
    "# We have decided not to include district of columbia in our state mappings, as it is not technically a state.\n",
    "states_map = states_map[states_map['State_Name'] != 'DISTRICT OF COLUMBIA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom map plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "\n",
    "def plot_us_states(gdf, data_column=None, plot_points=None, title='INSERT TITLE', colormap='RdYlGn', point_color='black', point_size=3, point_opacity=1, label_states=False, colorbar_title=None, log_scale=False, output_file=None):\n",
    "    fig = plt.figure(figsize=(20, 13))\n",
    "    fig.suptitle(title, fontsize=24)\n",
    "    \n",
    "    # Exclude Hawaii and Alaska when making the initial mainland/contiguous plot\n",
    "    mainland_us = gdf[~gdf['State_Name'].isin(['HAWAII', 'ALASKA'])]\n",
    "    # Make new geodataframes for each of the excluded states. We dont re-include\n",
    "    alaska = gdf[gdf['State_Name'] == 'ALASKA']\n",
    "    hawaii = gdf[gdf['State_Name'] == 'HAWAII']\n",
    "\n",
    "# Normalize the color map based on the entire dataset\n",
    "    if data_column:\n",
    "        if log_scale:\n",
    "            #if user is choosing to plot on a logarithmic scale, normalize the data values logarithmically \n",
    "            norm = mcolors.LogNorm(vmin=gdf[data_column].replace(0, np.nan).min(), vmax=gdf[data_column].max())\n",
    "        else:\n",
    "            # if user hasnt specified logarithmi scale, normalize the values as normal\n",
    "            norm = mcolors.Normalize(vmin=gdf[data_column].min(), vmax=gdf[data_column].max())\n",
    "\n",
    "    # Create a new axis for mainland US\n",
    "    ax_mainland = fig.add_axes([0, 0.05, 1, 1])  # [x, y, width, height]\n",
    "    if data_column:\n",
    "        # Plot mainland US with data column\n",
    "        mainland_us.plot(column=data_column, cmap=colormap, norm=norm, linewidth=0.8, ax=ax_mainland, edgecolor='0.5', legend=False, missing_kwds={'color': 'lightgrey'})\n",
    "    else:\n",
    "        # Plot mainland US without data column (all states in gray)\n",
    "        mainland_us.plot(color='lightgrey', linewidth=0.8, ax=ax_mainland, edgecolor='0.5')\n",
    "\n",
    "    # Add Alaska Axis (x, y, width, height)\n",
    "    akax = fig.add_axes([-0.05, 0.1, 0.3, 0.3])\n",
    "    if data_column:\n",
    "        # Plot Alaska with data column\n",
    "        alaska.plot(column=data_column, cmap=colormap, norm=norm, linewidth=0.8, ax=akax, edgecolor='0.5', missing_kwds={'color': 'lightgrey'})\n",
    "    else:\n",
    "        # Plot Alaska without data column (all states in gray)\n",
    "        alaska.plot(color='lightgrey', linewidth=0.8, ax=akax, edgecolor='0.5')\n",
    "    akax.set_title('Alaska', fontsize=20)\n",
    "    \n",
    "    # Add Hawaii Axis (x, y, width, height)\n",
    "    hiax = fig.add_axes([0.225, 0.15, 0.15, 0.15])\n",
    "    if data_column:\n",
    "        # Plot Hawaii with data column\n",
    "        hawaii.plot(column=data_column, cmap=colormap, norm=norm, linewidth=0.8, ax=hiax, edgecolor='0.5', missing_kwds={'color': 'lightgrey'})\n",
    "    else:\n",
    "        # Plot Hawaii without data column (all states in gray)\n",
    "        hawaii.plot(color='lightgrey', linewidth=0.8, ax=hiax, edgecolor='0.5')\n",
    "    hiax.set_title('Hawaii', fontsize=20)\n",
    "\n",
    "    # Hide all axes borders and labels\n",
    "    ax_mainland.set_axis_off()\n",
    "    akax.set_axis_off()\n",
    "    hiax.set_axis_off()\n",
    "\n",
    "    # If plotting datacenter locations, make a new geodataframe for datacenter point locations for the mainland, Alaska and Hawaii\n",
    "    if plot_points is not None:\n",
    "        # Exclude Hawaii and Alaska when making the initial mainland/contiguous plot\n",
    "        mainland_us_dc = plot_points[~plot_points['state'].isin(['Hawaii', 'Alaska'])]\n",
    "        # We only have to make a datacenter gdf for Hawaii, since Alaska doesn't have any datacenters of its own!\n",
    "        # again we dont include district of columbia\n",
    "        hawaii_dc = plot_points[plot_points['state'] == 'Hawaii']\n",
    "\n",
    "        # Plot the data points on the mainland map\n",
    "        mainland_us_dc.plot(ax=ax_mainland, color=point_color, markersize=point_size, alpha=point_opacity)\n",
    "        \n",
    "        # Plot the data points on the Hawaii map\n",
    "        hawaii_dc.plot(ax=hiax, color=point_color, markersize=point_size, alpha=point_opacity)\n",
    "\n",
    "    if label_states and data_column:\n",
    "        label_size = 15\n",
    "        for idx, row in mainland_us.iterrows():\n",
    "            #f\"{row[data_column]:.2f}\".rstrip('0').rstrip('.') rounds to 2 decimal places, and if the float value is an integer value, e.g. 23.00, we strip the 0's\n",
    "            ax_mainland.text(row.geometry.centroid.x, row.geometry.centroid.y, \n",
    "                             s=f\"{row[data_column]:.2f}\".rstrip('0').rstrip('.'), horizontalalignment='center', fontsize=label_size)\n",
    "\n",
    "        if not alaska.empty:\n",
    "            row = alaska.iloc[0]\n",
    "            akax.text(row.geometry.centroid.x, row.geometry.centroid.y, \n",
    "                      s=f\"{row[data_column]:.2f}\".rstrip('0').rstrip('.'), horizontalalignment='center', fontsize=label_size)\n",
    "        \n",
    "        if not hawaii.empty:\n",
    "            row = hawaii.iloc[0]\n",
    "            hiax.text(row.geometry.centroid.x, row.geometry.centroid.y, \n",
    "                      s=f\"{row[data_column]:.2f}\".rstrip('0').rstrip('.'), horizontalalignment='center', fontsize=label_size)\n",
    "\n",
    "    if data_column:\n",
    "        # Create an axis for the colorbar\n",
    "        cax = fig.add_axes([0.05, 0.07, 0.9, 0.02])  # [x, y, width, height]\n",
    "        # Create a ScalarMappable for the colorbar based on the colormap and normalization of the data values.\n",
    "        sm = plt.cm.ScalarMappable(cmap=colormap, norm=norm)\n",
    "        sm._A = []  # This is needed for ScalarMappable\n",
    "        # Give the colorbar a title named after the data column we are plotting\n",
    "        if colorbar_title == None:\n",
    "           colorbar_title = data_column\n",
    "        cax.set_title(colorbar_title, fontsize=20)\n",
    "        cax.tick_params(labelsize=20)\n",
    "        fig.colorbar(sm, cax=cax, orientation='horizontal')\n",
    "\n",
    "    if output_file is not None:\n",
    "        plt.savefig(os.path.join(out_path, output_file))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data center locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_us_states(states_map, \n",
    "               title='Datacenter Locations in the United States', \n",
    "               plot_points=dc_gdf, \n",
    "               point_color='red',\n",
    "               point_size=2,\n",
    "               output_file='data center locations.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uppercase_df(input_df):\n",
    "    return input_df.map(lambda x: x.upper() if isinstance(x, str) else x)\n",
    "\n",
    "def remove_trailing_space_df(input_df):\n",
    "    return input_df.map(lambda x: x.strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data center count by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_counts = dc_df['state'].value_counts().reset_index()\n",
    "# Convert all string values in the dataframe to uppercase since the shapefile uses uppercase state names\n",
    "state_counts = uppercase_df(state_counts)\n",
    "state_counts.columns = ['State_Name', 'Datacenter_Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the state counts with the state shapefile map based on state names\n",
    "merged_data = states_map.merge(state_counts, how='left', on='State_Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_column = 'Datacenter_Count'\n",
    "\n",
    "title = f'Datacenter Occurrences Heatmap'\n",
    "\n",
    "plot_us_states(merged_data, \n",
    "               data_column, \n",
    "               title=title, \n",
    "               colormap='OrRd', \n",
    "               label_states=True,\n",
    "               output_file='data center heatmap.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Power Pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_df = pd.read_excel('../in/avgprice_annual.xlsx', skiprows=1)\n",
    "\n",
    "#only keeping power prices from 2020\n",
    "power_df = power_df[power_df['Year'] == 2020]\n",
    "\n",
    "#only keeping \"total electric industry\" as the industry sector as its the only one that has all 51 states.\n",
    "power_df = power_df[power_df['Industry Sector Category'] == 'Total Electric Industry']\n",
    "\n",
    "power_df.rename(columns={'State': 'State_Code',\n",
    "                         'Industrial': 'Industrial power price'}, inplace=True)\n",
    "\n",
    "# Only keeping the columns of the state code and the \n",
    "power_df = power_df[['State_Code','Industrial power price']]\n",
    "\n",
    "power_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the power_df into the merged_data.\n",
    "#Merge them based on the \"state_code\" colmuns\n",
    "merged_data = merged_data.merge(power_df, how='left', on='State_Code')\n",
    "\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_sector = 'Industrial power price'\n",
    "\n",
    "title = f'Heatmap of {power_sector} Power Prices &\\n Data Center Locations in the United States'\n",
    "\n",
    "plot_us_states(merged_data, power_sector, \n",
    "               title=title, \n",
    "               colorbar_title='Industrial Power Pricing',\n",
    "               label_states=True,\n",
    "               plot_points=dc_gdf, \n",
    "               point_color='red',\n",
    "               colormap='RdYlGn_r',\n",
    "               output_file='industrial power price and datacenter locations.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internet speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internet_df = pd.read_csv('../in/internet_speed_by_region.csv')\n",
    "\n",
    "internet_df.rename(columns={'Region': 'State_Name'}, inplace=True)\n",
    "\n",
    "internet_df = uppercase_df(internet_df)\n",
    "\n",
    "internet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.merge(internet_df, how='left', on='State_Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_column = 'Internet Speed'\n",
    "\n",
    "title = f'Heatmap of average Internet Speeds in the United States\\n& Data center locations in the United States.'\n",
    "\n",
    "plot_us_states(merged_data, data_column, \n",
    "               title=title, \n",
    "               colorbar_title='Avg. Internet Speed (Mbps)', \n",
    "               colormap='RdYlBu', \n",
    "               label_states=True,\n",
    "               plot_points=dc_gdf, \n",
    "               point_color='green',\n",
    "               output_file='avg internet speed and data center locations.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the average annual temperature by states for 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_temp_file = os.path.join('..','in','state_temp.csv')\n",
    "state_temp = pd.read_csv(state_temp_file, skiprows=4)\n",
    "state_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we rename the \"name\" colum to \"State_Name\" and the \"value\" column to \"temperature\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_temp.rename(columns={'Name' : 'State_Name', 'Value' : 'temperature'}, inplace=True)\n",
    "\n",
    "state_temp = state_temp[['State_Name','temperature']]\n",
    "\n",
    "#converting to uppercase again, because thats what the merged_data shapefile uses for state names.\n",
    "state_temp = uppercase_df(state_temp)\n",
    "\n",
    "state_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding missing temperatures manually to each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the average of the annual high temperature and annual low temperature for Honolulu, Hawaii\n",
    "avg_temp_hawaii = (84+71)/2\n",
    "\n",
    "# Creates a data frame with the missing states and their average tempeatures \n",
    "missing_states = pd.DataFrame({\n",
    "    'State_Name': ['ALASKA', 'DISTRICT OF COLUMBIA', 'HAWAII'],\n",
    "    'temperature': [28.4, 61.2, avg_temp_hawaii]\n",
    "})\n",
    "\n",
    "state_temp = pd.concat([state_temp, missing_states])\n",
    "\n",
    "state_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we convert the temperatures from fahrenheit to celsius by applying a custom funtion to the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fahrenheit_to_celsius(fahrenheit):\n",
    "    \"\"\"\n",
    "    Convert Fahrenheit to Celsius.\n",
    "\n",
    "    Parameters:\n",
    "    fahrenheit (float): Temperature in Fahrenheit.\n",
    "\n",
    "    Returns:\n",
    "    float: Temperature in Celsius.\n",
    "    \"\"\"\n",
    "    celsius = (fahrenheit - 32) * 5.0/9.0\n",
    "    return celsius\n",
    "\n",
    "state_temp['temperature'] = state_temp['temperature'].apply(fahrenheit_to_celsius)\n",
    "\n",
    "state_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.merge(state_temp, how='left', on='State_Name')\n",
    "\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_column = 'temperature'\n",
    "\n",
    "title = f'Heatmap of average annual temperature (Â°C) & \\n Datacenter locations in the united states'\n",
    "\n",
    "plot_us_states(merged_data, \n",
    "               data_column, \n",
    "               plot_points=dc_gdf, \n",
    "               point_color='green',\n",
    "               title=title, \n",
    "               colormap='RdYlBu_r',\n",
    "               label_states=True,\n",
    "               output_file='avg temperatures and datacenter locations.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humidity_file = os.path.join('..','in','most-humid-states-2024.csv')\n",
    "state_humidity = pd.read_csv(humidity_file)\n",
    "state_humidity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_humidity.rename(columns={'state' : 'State_Name',\n",
    "                               'MostHumidStatesAverageRelativeHumidity' : 'relative_humidity',\n",
    "                               'MostHumidStatesAverageDewPointF' : 'avg_dewpoint'},\n",
    "                               inplace=True)\n",
    "state_humidity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting State_Name column to uppercase, and converting dewpoint from fahrenheit to celsius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_humidity = uppercase_df(state_humidity)\n",
    "\n",
    "state_humidity['avg_dewpoint'] = state_humidity['avg_dewpoint'].apply(fahrenheit_to_celsius)\n",
    "\n",
    "state_humidity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.merge(state_humidity, how='left', on='State_Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_column = 'relative_humidity'\n",
    "\n",
    "title = f'Heatmap of average annual relative humidity & \\n Datacenter locations in the united states'\n",
    "\n",
    "plot_us_states(merged_data, data_column, \n",
    "               plot_points=dc_gdf, \n",
    "               title=title, \n",
    "               label_states=False,\n",
    "               colorbar_title='Relative Humidity (%)',\n",
    "               colormap='rainbow_r',\n",
    "               output_file='avg humidity and datacenter locations.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Population density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_density_file = os.path.join('..','in','united-states-by-density-2024.csv')\n",
    "pop_density = pd.read_csv(pop_density_file)\n",
    "pop_density.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_density.rename(columns={'state' : 'State_Name'},\n",
    "                            inplace=True)\n",
    "pop_density = uppercase_df(pop_density)\n",
    "pop_density.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.merge(pop_density, how='left', on='State_Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_column = 'densityMi'\n",
    "\n",
    "title = f'Heatmap of population density by state'\n",
    "\n",
    "plot_us_states(merged_data,\n",
    "               data_column,\n",
    "               title=title,\n",
    "               colormap='OrRd',\n",
    "               label_states=True,\n",
    "               log_scale=True,\n",
    "               output_file='log population density.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renewable energy by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_states_file = os.path.join('..','in','green_states.csv')\n",
    "green_states = pd.read_csv(green_states_file)\n",
    "green_states.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_states.rename(columns={'State' : 'State_Name'},\n",
    "                            inplace=True)\n",
    "green_states = uppercase_df(green_states)\n",
    "green_states = remove_trailing_space_df(green_states)\n",
    "green_states=green_states[['State_Name','Renewable']]\n",
    "green_states.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_states['Renewable'] = green_states['Renewable'].str.rstrip('%').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.merge(green_states, how='left', on='State_Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_column = 'Renewable'\n",
    "\n",
    "title = f'How much of a states energy production comes from renewable sources (%)'\n",
    "\n",
    "plot_us_states(merged_data,\n",
    "               data_column,\n",
    "               title=title,\n",
    "               colormap='RdYlGn',\n",
    "               label_states=True,\n",
    "               output_file='renewable energy by state.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save statistics from the merged data as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['State_Name', 'Datacenter_Count', 'Industrial power price', 'Internet Speed', 'temperature', 'relative_humidity', 'densityMi', 'Renewable']\n",
    "\n",
    "state_stats = merged_data[columns_to_keep]\n",
    "\n",
    "state_stats\n",
    "\n",
    "state_stats.to_csv('../out/state_stats.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
